import Bijectors: VecCorrBijector, VecCholeskyBijector, inverse, with_logabsdet_jacobian, bijector
import Turing.DistributionsAD: TuringDenseMvNormal, TuringDiagMvNormal
import Zygote.ChainRules: @ignore_derivatives
import LinearAlgebra: Diagonal
import ForwardDiff

using Distributions

"""
The ELBO we use is:

    â„’(ğœ™) = ğ”¼[log p(y | X(Tâ»Â¹(Î·))) + log p(Î·)] - ğ”¼[q_ğœ™(Î· | y)]
         = ğ”¼[log p(y | X(Tâ»Â¹(Î·))) + log p(Î·)] + â„[q_ğœ™] 

This gives an approximation of the posterior from the MCMC procedure. It is 
however possible that the way we code the MCMC model is incorrect (i.e. maybe 
y ~ MultivariateNormal is not correct?).
"""

# We use:
# * MultivariateNormal for p(y | z; ÏƒÂ²), 
# * MultivariateNormal for p(Î·; Î©), 
# * Full-rank approximation for q(Î·; ğœ™)

variance(yÌ‚, Ïƒ::T) where T<:Real = Diagonal(fill(Ïƒ^2, length(yÌ‚))) # Additive error (or Proportional)
variance(yÌ‚, Ïƒ::AbstractVector{T}) where T<:Real = Diagonal((Ïƒ[1] .+ (yÌ‚ .* Ïƒ[2])).^2) # Combined error

function loglikelihood(prob, individual, z, Ïƒ)
    yÌ‚ = predict_adjoint(prob, individual, z)
    Î£ = variance(yÌ‚, Ïƒ)
    return logpdf(MultivariateNormal(yÌ‚, Î£), individual.y)
end

logprior(Î·::AbstractVector, Î©::AbstractMatrix) = logpdf(MultivariateNormal(zero.(Î·), Î©), Î·)

logjoint(prob, individual, z, Î·, Î©, Ïƒ) = loglikelihood(prob, individual, z, Ïƒ) + logprior(Î·, Î©)

# More expensive on a population basis (probably due to inversion of Î©)
# function emp_bayes_est!(population, prob, model, p, st)
#     Ïƒ = softplus(p.theta[1])
#     Ï‰ = softplus.(p.theta[2:3])
#     C = inverse(VecCorrBijector())(p.theta[4:end])
#     Î© = Symmetric(Ï‰ .* C .* Ï‰')
#     Î¶, _ = Lux.apply(model, population.x, p.weights, st)

#     for i in eachindex(population)
#         opt = Optim.optimize(eta -> -2 * logjoint(prob, population[i], Î¶[:, i] .* exp.([1 0; 0 1; 0 0; 0 0] * eta), eta, Î©, Ïƒ), zero.(population[i].eta))
#         population[i].eta .= Float32.(opt.minimizer)
#     end

#     nothing
# end

# logabsdetjac_z(z::AbstractVector{T}) where {T<:Real} = log(abs(det(Diagonal(-one(T) ./ z))))

function variational_posterior(getq, ğœ™áµ¢::AbstractVector{T}, Î·; path_deriv_est = true) where {T<:ForwardDiff.Dual}
    q = path_deriv_est ? getq(map(ğœ™ -> ğœ™.value, ğœ™áµ¢)) : getq(ğœ™áµ¢) # @ignore_derivatives does not work for ForwardDiff.
    return variational_posterior(q, Î·)
end

function variational_posterior(getq, ğœ™áµ¢, Î·; path_deriv_est = true) # Evaluates q_ğœ™(z)
    q = !path_deriv_est ? getq(ğœ™áµ¢) : @ignore_derivatives getq(ğœ™áµ¢) # path derivative gradient estimator from "Sticking the landing ... "
    # The path derivative estimator is a bad choice when the variational posterior does not match the true posterior.
    return variational_posterior(q, Î·)
end

variational_posterior(q::Distribution, Î·) = logpdf(q, Î·) # OLD: + logabsdetjac_z(z)

function elbo(prob, individual::Individual, Î¶áµ¢, getq::Function, ğœ™áµ¢, Î©, Ïƒ; with_entropy)
    Î·áµ¢ = rand(getq(ğœ™áµ¢))
    záµ¢ = Î¶áµ¢ .* exp.([1 0; 0 1; 0 0; 0 0] * Î·áµ¢)
    logÏ€ = logjoint(prob, individual, záµ¢, Î·áµ¢, Î©, Ïƒ)
    return with_entropy ? logÏ€ + entropy(getq(ğœ™áµ¢)) : logÏ€ - variational_posterior(getq, ğœ™áµ¢, Î·áµ¢)
end

function mean_field_approx(ğœ™) # ğœ™ = [Î¼, Ïƒ*]
    d = Integer(length(ğœ™) / 2)
    Î¼ = ğœ™[1:d]
    Ïƒ = softplus.(ğœ™[d+1:d*2])
    return MultivariateNormal(Î¼, Ïƒ)
end

function full_rank(ğœ™; d=2) # ğœ™ = [Î¼, Ï‰*, Ï*], d = num_rand_effects
    Î¼ = ğœ™[1:d]
    Ïƒ = softplus.(ğœ™[d+1:2d])
    Ï = tanh.(ğœ™[2d+1:end])
    Î£ = covariance_matrix(Ï, Ïƒ)
    return MultivariateNormal(Î¼, Î£)
end

J_sigmoid(x::T) where {T<:Real} = sigmoid(x) * (one(T) - sigmoid(x))

function getÎ¸(Î¸_; d) # Î¸_ = {Ï‰, Ï, Ïƒ}
    Ï‰ = exp.(Î¸_[1:d])
    Ï_ = Î¸_[d+1:d+sum(1:d-1)]
    Ï = sigmoid.(1 .+ 1 .- exp.(Ï_)) # Makes the distribution left skewed between 0 and 1
    Ïƒ = exp.(Î¸_[d+sum(1:d-1)+1:end])
    logabsdet = log(abs(det(Diagonal(Ï‰)))) + log(abs(det(Diagonal(J_sigmoid.(exp.(Ï)) .* exp.(Ï))))) + log(abs(det(Diagonal(Ïƒ))))
    return Ï‰, Ï, length(Ïƒ) == 1 ? Ïƒ[1] : Ïƒ, logabsdet
end

"""Calculates the posterior over Î·"""
function partial_advi(ann, prob, population, ğœ™::NamedTuple, st; num_samples = 1, with_entropy = false)
    m = 2 # num_rand_effects
    Î¶, _ = Lux.apply(ann, population.x, ğœ™.weights, st) # Lux
    Ïƒ_Ï‰ = softplus.(ğœ™.theta[1:3])
    Ïƒ = Ïƒ_Ï‰[1]
    Ï‰ = Ïƒ_Ï‰[2:3]
    C = inverse(VecCorrBijector())(ğœ™.theta[4:end])
    Î© = Symmetric(Ï‰ .* C .* Ï‰')

    ELBO = zero(eltype(Î¶))
    for _ in 1:num_samples
        for i in eachindex(population)
            # Calculates p(y | Î·; Ïƒ) + p(Î·; Î©) - q_ğœ™(Î·)
            Láµ¢ = LowerTriangular(_chol_lower(inverse(VecCholeskyBijector(:L))(ğœ™.phi[2m+1:end, i])) .* softplus.(ğœ™.phi[m+1:2m, i]))
            Î·áµ¢ = ğœ™.phi[1:m, i] + Láµ¢ * randn(Float32, m)
            záµ¢ = Î¶[:, i] .* exp.([1 0; 0 1; 0 0; 0 0] * Î·áµ¢)
            logÏ€ = logjoint(prob, population[i], záµ¢, Î·áµ¢, Î©, Ïƒ)
            qáµ¢ = @ignore_derivatives MultivariateNormal(ğœ™.phi[1:2, i], Láµ¢ * Láµ¢')
            ELBO += (logÏ€ - logpdf(qáµ¢, Î·áµ¢)) / num_samples
        end
    end
    
    return ELBO
end

function partial_advi_prop(ann, prob, population, ğœ™::NamedTuple, st; num_samples = 1, with_entropy = false)
    m = 2 # num_rand_effects
    Î¶, _ = Lux.apply(ann, Zygote.ignore_derivatives(population.x), ğœ™.weights, st) # Lux
    Ïƒ_Ï‰ = softplus.(ğœ™.theta[1:4])
    Ïƒ = Ïƒ_Ï‰[1:2]
    Ï‰ = Ïƒ_Ï‰[3:4]
    C = inverse(VecCorrBijector())(ğœ™.theta[5:end])
    Î© = Symmetric(Ï‰ .* C .* Ï‰')

    ELBO = zero(eltype(Î¶))
    for _ in 1:num_samples
        for i in eachindex(population)
            # Calculates p(y | Î·; Ïƒ) + p(Î·; Î©) - q_ğœ™(Î·)
            Láµ¢ = LowerTriangular(_chol_lower(inverse(VecCholeskyBijector(:L))(ğœ™.phi[2m+1:end, i])) .* softplus.(ğœ™.phi[m+1:2m, i]))
            Î·áµ¢ = ğœ™.phi[1:m, i] + Láµ¢ * randn(Float32, m)
            záµ¢ = Î¶[:, i] .* exp.([1 0; 0 1; 0 0; 0 0] * Î·áµ¢)
            logÏ€ = logjoint(prob, population[i], záµ¢, Î·áµ¢, Î©, Ïƒ)
            qáµ¢ = @ignore_derivatives MultivariateNormal(ğœ™.phi[1:2, i], Láµ¢ * Láµ¢')
            ELBO += (logÏ€ - logpdf(qáµ¢, Î·áµ¢)) / num_samples
        end
    end
    
    return ELBO
end

"""This formulation uses cholesky decompositions instead of Ï‰ and C"""
function partial_advi_alt(ann, prob, population, ğœ™::NamedTuple, st; num_samples = 3)
    m = 2 # num_rand_effects
    Î¶, _ = Lux.apply(ann, population.x, ğœ™.weights, st) # Lux
    Ïƒ = softplus.(ğœ™.theta[1])
    L = vec_to_tril(ğœ™.theta[2:end])
    Î© = L * L'

    ELBO = zero(eltype(Î¶))
    for _ in 1:num_samples
        for i in eachindex(population)
            # Calculates p(y | Î·; Ïƒ) + p(Î·; Î©) - q_ğœ™(Î·)
            Láµ¢ = vec_to_tril(ğœ™.phi[m+1:end, i])
            Î·áµ¢ = ğœ™.phi[1:m, i] + Láµ¢ * randn(Float32, m)
            záµ¢ = Î¶[:, i] .* exp.([1 0; 0 1; 0 0; 0 0] * Î·áµ¢)
            logÏ€ = logjoint(prob, population[i], záµ¢, Î·áµ¢, Î©, Ïƒ)
            qáµ¢ = @ignore_derivatives MultivariateNormal(ğœ™.phi[1:2, i], Láµ¢ * Láµ¢')
            ELBO += (logÏ€ - logpdf(qáµ¢, Î·áµ¢)) / num_samples
        end
    end
    
    return ELBO
    
    # m = 2 # num_rand_effects
    # Î¶, _ = Lux.apply(ann, population.x, ğœ™.weights, st) # Lux
    # Ïƒ = softplus(ğœ™.theta[1])
    # L = LowerTriangular(vec_to_tril(ğœ™.theta[2:end]))
    # Î© = L * L'

    # ELBO = eltype(Î¶)
    # for _ in 1:num_samples
    #     for i in eachindex(population)
    #         # Calculates p(y | Î·; Ïƒ) + p(Î·; Î©) - q_ğœ™(Î·)
    #         Láµ¢ = LowerTriangular(vec_to_tril(ğœ™.phi[m+1:end, i]))
    #         Î·áµ¢ = ğœ™.phi[1:m, i] + Láµ¢ * randn(Float32, m)
    #         záµ¢ = Î¶[:, i] .* exp.([1 0; 0 1; 0 0; 0 0] * Î·áµ¢)
    #         logÏ€ = logjoint(prob, population[i], záµ¢, Î·áµ¢, Î©, Ïƒ)
    #         qáµ¢ = @ignore_derivatives MultivariateNormal(ğœ™.phi[1:m, i], Láµ¢ * Láµ¢')
    #         ELBO += (logÏ€ - logpdf(qáµ¢, Î·áµ¢)) / num_samples
    #     end
    # end
    
    # return ELBO
end

# Lc ~ LKJCholesky() â† use Bijector and set LKJCholesky prior on Lc
# Ï‰ ~ LogNormal
# L = LowerTriangular(Lc .* Ï‰) == cholesky(Î©)
_chol_lower(a::Cholesky) = a.uplo === 'L' ? a.L : a.U'

"""Calculates the posterior over Î·, as well as over Ï‰, Ï (reasonable approximation), and Ïƒ"""
function full_advi(ann, prob, population, ğœ™::NamedTuple, st; num_samples = 3, with_entropy = false)
    Î¶, _ = ann(population.x, ğœ™.weights, st)
    b1 = inverse(VecCorrBijector())
    b2 = inverse(VecCholeskyBijector(:L))
    b3 = inverse(bijector(LogNormal()))
    d = 4 # num_theta
    m = 2 # num_rand_effects
    
    Î¸_Î¼ = ğœ™.theta[1:d]
    Î¸_Lc = LowerTriangular(_chol_lower(b2(ğœ™.theta[2d+1:end])) .* softplus.(ğœ™.theta[d+1:2d]))
    q_Î¸ = @ignore_derivatives MultivariateNormal(Î¸_Î¼, Î¸_Lc * Î¸_Lc')
    
    ELBO = zero(eltype(Î¶))
    for _ in 1:num_samples
        Î¸ = Î¸_Î¼ + Î¸_Lc * randn(Float32, d)
        Ïƒ_Ï‰, logabsdetjac1 = with_logabsdet_jacobian(b3, Î¸[1:3]) # exp
        Ïƒ = Ïƒ_Ï‰[1]
        Ï‰ = Ïƒ_Ï‰[2:3]
        C, logabsdetjac2 = with_logabsdet_jacobian(b1, Î¸[4:end]) # vec â†’ C
        logabsdetjac_theta = logabsdetjac1 + logabsdetjac2

        Î© = Symmetric(Ï‰ .* C .* Ï‰')
        
        for i in eachindex(population)
            # Calculates p(y | Î·) + p(Î·) - q_ğœ™(Î·)
            Láµ¢ = LowerTriangular(_chol_lower(b2(ğœ™.phi[2m+1:end, i])) .* softplus.(ğœ™.phi[m+1:2m, i]))
            Î·áµ¢ = ğœ™.phi[1:m, i] + Láµ¢ * randn(Float32, m)
            záµ¢ = Î¶[:, i] .* exp.([1 0; 0 1; 0 0; 0 0] * Î·áµ¢)
            logÏ€ = logjoint(prob, population[i], záµ¢, Î·áµ¢, Î©, Ïƒ)
            qáµ¢ = @ignore_derivatives MultivariateNormal(ğœ™.phi[1:2, i], Láµ¢ * Láµ¢')
            ELBO += (logÏ€ - logpdf(qáµ¢, Î·áµ¢)) / num_samples
        end
        
        priors = logpdf(LogNormal(-3.f0, 1.f0), Ïƒ) + sum(logpdf.(LogNormal(-1.5f0, 1.f0), Ï‰)) + logpdf(LKJ(2, 2.f0), C) + logabsdetjac_theta
        ELBO += (priors / num_samples) # p(Ï‰)
        ELBO -= (logpdf(q_Î¸, Î¸) / num_samples) # log(abs(det(J_Î¸))) + { â„[q_ğœ™] or -q_ğœ™(Î¸) }
    end
    
    return ELBO
end
